import { EventEmitter } from 'events';
import { DbtRunner } from './DbtRunner';
import { ManifestParser } from '../parsers/ManifestParser';
import { EnhancedSQLParser } from '../parsers/EnhancedSQLParser';
import { PythonSQLGlotParser } from '../parsers/PythonSQLGlotParser';
import { supabase } from '../../../config/supabase';
import { ExtractionLogger } from '../../../utils/ExtractionLogger';
import { TantivySearchService } from '../../TantivySearchService';

export enum ExtractionPhase {
  QUEUED = 'queued',
  CLONING = 'cloning',
  INSTALLING_DEPS = 'installing_deps',
  PARSING = 'parsing',
  STORING = 'storing',
  COMPLETED = 'completed',
  FAILED = 'failed'
}

export interface ExtractionProgress {
  connectionId: string;
  phase: ExtractionPhase;
  progress: number; // 0-100
  message: string;
  startTime: Date;
  currentStepStartTime: Date;
  errors: string[];
}

export interface ExtractionResult {
  success: boolean;
  connectionId: string;
  models: number;
  sources: number;
  dependencies: number;
  columnLineage: number;
  duration: number;
  errors: string[];
}

export class ExtractionOrchestrator extends EventEmitter {
  private dbtRunner: DbtRunner;
  private manifestParser: ManifestParser;
  private sqlParser: EnhancedSQLParser;
  private pythonParser: PythonSQLGlotParser;
  private activeExtractions: Map<string, ExtractionProgress>;

  constructor() {
    super();
    this.dbtRunner = new DbtRunner();
    this.manifestParser = new ManifestParser();
    this.sqlParser = new EnhancedSQLParser();
    this.pythonParser = new PythonSQLGlotParser();
    this.activeExtractions = new Map();
  }

  /**
   * Normalize table name by removing quotes but preserving schema/database prefixes
   * Returns multiple lookup keys to try:
   * Examples:
   *   "dummy"."main"."stg_transaction" -> ["dummy.main.stg_transaction", "main.stg_transaction", "stg_transaction"]
   *   dummy.main.stg_transaction -> ["dummy.main.stg_transaction", "main.stg_transaction", "stg_transaction"]
   *   __corporation_industry -> ["__corporation_industry"]
   */
  private getTableNameVariants(tableName: string): string[] {
    // Remove all quotes
    const cleaned = tableName.replace(/"/g, '');
    
    // Split by dots
    const parts = cleaned.split('.');
    
    if (parts.length === 1) {
      // Simple name: "stg_transaction"
      return [cleaned.trim()];
    } else if (parts.length === 2) {
      // Schema.table: "main.stg_transaction"
      return [
        cleaned.trim(),           // "main.stg_transaction"
        parts[1].trim()          // "stg_transaction"
      ];
    } else if (parts.length >= 3) {
      // Database.schema.table: "dummy.main.stg_transaction"
      return [
        cleaned.trim(),                        // "dummy.main.stg_transaction"
        `${parts[parts.length - 2]}.${parts[parts.length - 1]}`.trim(),  // "main.stg_transaction"
        parts[parts.length - 1].trim()        // "stg_transaction"
      ];
    }
    
    return [cleaned.trim()];
  }

  /**
   * Start metadata extraction for a connection
   */
  async startExtraction(connectionId: string): Promise<ExtractionResult> {
    console.log(`\n${'='.repeat(60)}`);
    console.log(`üöÄ Starting extraction for connection: ${connectionId}`);
    console.log(`${'='.repeat(60)}\n`);

    const startTime = new Date();

    // Initialize progress tracking
    const progress: ExtractionProgress = {
      connectionId,
      phase: ExtractionPhase.QUEUED,
      progress: 0,
      message: 'Extraction queued',
      startTime,
      currentStepStartTime: startTime,
      errors: []
    };

    this.activeExtractions.set(connectionId, progress);
    this.emitProgress(progress);

    try {
      // Update connection status
      await this.updateConnectionStatus(connectionId, 'extracting');

      // Get connection details
      const connection = await this.getConnection(connectionId);
      if (!connection) {
        throw new Error('Connection not found');
      }

      // Phase 1: Clone repository
      await this.updatePhase(connectionId, ExtractionPhase.CLONING, 10, 'Cloning repository...');
      
      const dbtResult = await this.dbtRunner.extractMetadata(
        connection.repository_url,
        connection.branch,
        connection.access_token_encrypted, // TODO: Decrypt
        connection.provider || 'github' // Pass provider to support GitLab
      );

      if (!dbtResult.success) {
        throw new Error(`dbt parse failed: ${dbtResult.errors.join(', ')}`);
      }

      // Phase 2: Parse manifest
      await this.updatePhase(connectionId, ExtractionPhase.PARSING, 60, 'Parsing manifest...');
      
      const parsed = await this.manifestParser.parseManifest(
        JSON.stringify(dbtResult.manifest)
      );

      console.log(`üìä Parsed manifest:`);
      console.log(`   Models: ${parsed.models.length}`);
      console.log(`   Sources: ${parsed.sources.length}`);
      console.log(`   Dependencies: ${parsed.dependencies.length}`);
      console.log(`   Column Lineage: ${parsed.columnLineage.length}`);

      // Phase 3: Store in database
      await this.updatePhase(connectionId, ExtractionPhase.STORING, 80, 'Storing in database...');
      
      await this.storeManifestData(connectionId, connection.organization_id, parsed);

      // Mark as complete
      await this.updatePhase(connectionId, ExtractionPhase.COMPLETED, 100, 'Extraction completed');
      
      await this.updateConnectionStatus(connectionId, 'completed');
      await this.markManifestUploaded(
        connectionId,
        dbtResult.manifest.metadata?.dbt_schema_version || 'unknown',
        dbtResult.manifest.metadata?.dbt_version || 'unknown'
      );

      // Trigger Tantivy search indexing (async, non-blocking)
      console.log(`üöÄ Starting Tantivy indexing for org: ${connection.organization_id}`);
      TantivySearchService.getInstance()
        .triggerIndexing(connection.organization_id)
        .catch(err => {
          console.error('‚ùå Search indexing error:', err);
          console.error('   Error details:', err instanceof Error ? err.stack : err);
        });

      const duration = Date.now() - startTime.getTime();

      const result: ExtractionResult = {
        success: true,
        connectionId,
        models: parsed.models.length,
        sources: parsed.sources.length,
        dependencies: parsed.dependencies.length,
        columnLineage: parsed.columnLineage.length,
        duration,
        errors: []
      };

      console.log(`\n${'='.repeat(60)}`);
      console.log(`‚úÖ Extraction completed successfully`);
      console.log(`   Duration: ${duration}ms`);
      console.log(`${'='.repeat(60)}\n`);

      this.activeExtractions.delete(connectionId);
      this.emit('extraction-complete', result);

      return result;

    } catch (error: any) {
      console.error(`\n‚ùå Extraction failed:`, error.message);

      await this.updatePhase(
        connectionId,
        ExtractionPhase.FAILED,
        0,
        `Extraction failed: ${error.message}`
      );

      await this.updateConnectionStatus(connectionId, 'failed', error.message);

      const result: ExtractionResult = {
        success: false,
        connectionId,
        models: 0,
        sources: 0,
        dependencies: 0,
        columnLineage: 0,
        duration: Date.now() - startTime.getTime(),
        errors: [error.message]
      };

      this.activeExtractions.delete(connectionId);
      this.emit('extraction-failed', result);

      throw error;
    }
  }

  /**
   * Get extraction progress for a connection
   */
  getProgress(connectionId: string): ExtractionProgress | null {
    return this.activeExtractions.get(connectionId) || null;
  }

  /**
   * Get all active extractions
   */
  getActiveExtractions(): string[] {
    return Array.from(this.activeExtractions.keys());
  }

  /**
   * Cancel an active extraction (or reset failed/error status)
   */
  async cancelExtraction(connectionId: string): Promise<boolean> {
    // Check if extraction is in memory (actively running)
    const progress = this.activeExtractions.get(connectionId);
    
    // Also check database status
    const connection = await this.getConnection(connectionId);
    
    // Allow canceling if:
    // 1. Actively running (in memory)
    // 2. Database shows extracting/failed/error (allows reset)
    const cancelableStatuses = ['extracting', 'failed', 'error'];
    const canCancel = progress || cancelableStatuses.includes(connection.status);
    
    if (!canCancel) {
      console.log(`‚ö†Ô∏è  Cannot cancel extraction for: ${connectionId}`);
      console.log(`   Connection status: ${connection.status}`);
      console.log(`   Tip: Status must be 'extracting', 'failed', or 'error' to cancel`);
      return false;
    }

    console.log(`üõë Cancelling/resetting extraction for connection: ${connectionId}`);
    console.log(`   Current status: ${connection.status}`);

    // Clean up in-memory tracking if it exists
    if (progress) {
      await this.updatePhase(
        connectionId,
        ExtractionPhase.FAILED,
        0,
        'Extraction cancelled by user'
      );
    }

    // Always reset connection status to clean state
    await this.updateConnectionStatus(connectionId, 'connected', undefined);

    // Clean up in-memory state
    this.activeExtractions.delete(connectionId);

    this.emit('extraction-cancelled', { connectionId });

    console.log(`‚úÖ Extraction cancelled/reset to 'connected' status`);

    return true;
  }

  // Private helper methods

  private async updatePhase(
    connectionId: string,
    phase: ExtractionPhase,
    progress: number,
    message: string
  ): Promise<void> {
    const extraction = this.activeExtractions.get(connectionId);
    if (extraction) {
      extraction.phase = phase;
      extraction.progress = progress;
      extraction.message = message;
      extraction.currentStepStartTime = new Date();
      
      this.emitProgress(extraction);
    }

    console.log(`üìç [${progress}%] ${message}`);
  }

  private emitProgress(progress: ExtractionProgress): void {
    this.emit('progress', progress);
  }

  private async getConnection(connectionId: string): Promise<any> {
    const { data, error } = await supabase
      .schema('enterprise')
      .from('github_connections')
      .select('*')
      .eq('id', connectionId)
      .single();

    if (error) throw error;
    return data;
  }

  private async updateConnectionStatus(
    connectionId: string,
    status: string,
    errorMessage?: string
  ): Promise<void> {
    const updates: any = {
      status,
      updated_at: new Date().toISOString()
    };

    if (errorMessage) {
      updates.error_message = errorMessage;
    }

    await supabase
      .schema('enterprise')
      .from('github_connections')
      .update(updates)
      .eq('id', connectionId);
  }

  private async markManifestUploaded(
    connectionId: string,
    manifestVersion: string,
    dbtVersion: string
  ): Promise<void> {
    await supabase
      .schema('enterprise')
      .from('github_connections')
      .update({
        manifest_uploaded: true,
        manifest_version: manifestVersion,
        manifest_dbt_version: dbtVersion,
        extraction_tier: 'GOLD',
        last_extraction_at: new Date().toISOString(),
        updated_at: new Date().toISOString()
      })
      .eq('id', connectionId);
  }

  private async storeManifestData(
    connectionId: string,
    organizationId: string,
    parsed: any
  ): Promise<void> {
    console.log(`üíæ Storing manifest data for connection: ${connectionId}`);
    console.log(`   Organization: ${organizationId}`);
    console.log(`   Models to store: ${parsed.models.length}`);
    
    // Get or create repository in metadata schema
    const { data: repo, error: repoError } = await supabase
      .schema('metadata')
      .from('repositories')
      .select('id')
      .eq('connection_id', connectionId)
      .single();

    if (repoError && repoError.code !== 'PGRST116') {
      console.error('‚ùå Error fetching repository:', repoError);
      throw new Error(`Failed to fetch repository: ${repoError.message}`);
    }

    let repositoryId = repo?.id;

    if (!repositoryId) {
      console.log('üìÅ Creating new repository record...');
      const { data: newRepo, error: insertError } = await supabase
        .schema('metadata')
        .from('repositories')
        .insert({
          organization_id: organizationId,
          connection_id: connectionId,
          path: '/models',
          name: 'dbt_models',
          type: 'dbt_project'
        })
        .select('id')
        .single();
      
      if (insertError) {
        console.error('‚ùå Error creating repository:', insertError);
        throw new Error(`Failed to create repository: ${insertError.message}`);
      }
      
      repositoryId = newRepo?.id;
      console.log(`‚úÖ Repository created: ${repositoryId}`);
    } else {
      console.log(`‚úÖ Using existing repository: ${repositoryId}`);
      
      // CRITICAL FIX: Clear existing data before re-extraction to prevent duplicates
      // This matches IDE behavior and prevents duplicate objects/columns
      console.log(`üßπ Re-extraction detected - clearing existing data to prevent duplicates...`);
      await this.clearRepositoryData(repositoryId, organizationId);
    }

    // Store models
    console.log(`üì¶ Storing ${parsed.models.length} models...`);
    for (const model of parsed.models) {
      console.log(`   Processing model: ${model.name}`);
      
      const { data: file, error: fileError } = await supabase
        .schema('metadata')
        .from('files')
        .upsert({
          repository_id: repositoryId,
          organization_id: organizationId,
          connection_id: connectionId,
          relative_path: `models/${model.name}.sql`,
          absolute_path: `/models/${model.name}.sql`,
          file_type: 'dbt',
          dialect: 'sql',
          parser_used: 'dbt-manifest',
          parse_confidence: 1.0
        }, {
          onConflict: 'organization_id,absolute_path'
        })
        .select('id')
        .single();

      if (fileError) {
        console.error(`   ‚ùå Error creating file for ${model.name}:`, fileError);
        continue;
      }

      if (file) {
        console.log(`   ‚úÖ File created: ${file.id}`);
        
        const { data: object, error: objectError } = await supabase
          .schema('metadata')
          .from('objects')
          .insert({
            file_id: file.id,
            repository_id: repositoryId,
            organization_id: organizationId,
            connection_id: connectionId,
            name: model.name,
            schema_name: model.schema,
            database_name: model.database,
            object_type: model.object_type,
            definition: model.raw_sql,
            compiled_definition: model.compiled_sql,
            extracted_from: 'manifest',
            extraction_tier: 'GOLD',
            confidence: 1.0,
            metadata: {
              tags: model.tags,
              unique_id: model.unique_id,
              description: model.description
            }
          })
          .select('id')
          .single();

        if (objectError) {
          console.error(`   ‚ùå Error creating object for ${model.name}:`, objectError);
          continue;
        }
        
        console.log(`   ‚úÖ Object created: ${object?.id}`);

        if (object) {
          for (const col of model.columns) {
            await supabase
              .schema('metadata')
              .from('columns')
              .insert({
                object_id: object.id,
                organization_id: organizationId,
                name: col.name,
                data_type: col.data_type,
                position: col.position,
                description: col.description
              });
          }
        }
      }
    }

    // Store sources (similar pattern)
    for (const source of parsed.sources) {
      const { data: file } = await supabase
        .schema('metadata')
        .from('files')
        .upsert({
          repository_id: repositoryId,
          organization_id: organizationId,
          connection_id: connectionId,
          relative_path: `sources/${source.source_name}/${source.name}.yml`,
          absolute_path: `/sources/${source.source_name}/${source.name}.yml`,
          file_type: 'dbt',
          dialect: 'sql',
          parser_used: 'dbt-manifest',
          parse_confidence: 1.0
        }, {
          onConflict: 'organization_id,absolute_path'
        })
        .select('id')
        .single();

      if (file) {
        const { data: object } = await supabase
          .schema('metadata')
          .from('objects')
          .insert({
            file_id: file.id,
            repository_id: repositoryId,
            organization_id: organizationId,
            connection_id: connectionId,
            name: source.name,
            schema_name: source.schema,
            database_name: source.database,
            object_type: 'source',
            extracted_from: 'manifest',
            extraction_tier: 'GOLD',
            confidence: 1.0,
            metadata: {
              source_name: source.source_name,
              identifier: source.identifier,
              unique_id: source.unique_id
            }
          })
          .select('id')
          .single();

        if (object) {
          for (const col of source.columns) {
            await supabase
              .schema('metadata')
              .from('columns')
              .insert({
                object_id: object.id,
                organization_id: organizationId,
                name: col.name,
                data_type: col.data_type,
                description: col.description
              });
          }
        }
      }
    }

    // Store dependencies and build maps for lineage
    const objectMapByUniqueId = new Map<string, string>(); // unique_id ‚Üí object_id
    const objectMapByName = new Map<string, string>();      // name ‚Üí object_id
    const dependencyMap = new Map<string, string[]>();      // target_name ‚Üí [source_names]
    
    // Fetch ALL objects using pagination - critical for dependency mapping
    console.log(`üîç Fetching ALL objects for connection: ${connectionId}`);
    let allObjects: any[] = [];
    let page = 0;
    const pageSize = 1000;
    let hasMore = true;

    while (hasMore) {
      const from = page * pageSize;
      const to = from + pageSize - 1;
      
      const { data: pageData, error: objectFetchError } = await supabase
        .schema('metadata')
        .from('objects')
        .select('id, name, metadata')
        .eq('connection_id', connectionId)
        .range(from, to);

      if (objectFetchError) {
        console.error(`‚ùå Error fetching objects page ${page}:`, objectFetchError);
        break;
      }

      if (pageData && pageData.length > 0) {
        allObjects = allObjects.concat(pageData);
        console.log(`   üìÑ Page ${page + 1}: fetched ${pageData.length} objects (total: ${allObjects.length})`);
        hasMore = pageData.length === pageSize;
        page++;
      } else {
        hasMore = false;
      }
    }
    
    console.log(`üì¶ Fetched ${allObjects.length} total objects from database`);
    
    if (allObjects.length > 0) {
      for (const obj of allObjects) {
        // Map by unique_id for dependency storage
        const uniqueId = obj.metadata?.unique_id;
        if (uniqueId) {
          objectMapByUniqueId.set(uniqueId, obj.id);
        }
        
        // Map by name for lineage extraction (with schema support)
        // Add multiple lookup keys for better matching:
        // 1. Simple name: "stg_transaction"
        objectMapByName.set(obj.name, obj.id);
        
        // 2. Schema-qualified: "main.stg_transaction"
        if (obj.schema_name) {
          objectMapByName.set(`${obj.schema_name}.${obj.name}`, obj.id);
        }
        
        // 3. Database-qualified (if available): "dummy.main.stg_transaction"
        if (obj.database_name && obj.schema_name) {
          objectMapByName.set(`${obj.database_name}.${obj.schema_name}.${obj.name}`, obj.id);
        }
      }
    }

    console.log(`\nüìã Object map built:`);
    console.log(`   Total objects in DB: ${allObjects?.length || 0}`);
    console.log(`   Objects with unique_id: ${objectMapByUniqueId.size}`);
    console.log(`   Sample unique_ids (first 5):`);
    Array.from(objectMapByUniqueId.keys()).slice(0, 5).forEach(key => {
      console.log(`      - ${key}`);
    });

    // Store model dependencies and build dependency map
    console.log(`\nüîó Storing ${parsed.dependencies.length} dependencies from manifest...`);
    let storedCount = 0;
    let skippedSourceCount = 0;
    let skippedTargetCount = 0;
    const skippedSources = new Set<string>();
    const skippedTargets = new Set<string>();

    for (const dep of parsed.dependencies) {
      const sourceId = objectMapByUniqueId.get(dep.source_unique_id);
      const targetId = objectMapByUniqueId.get(dep.target_unique_id);

      if (!sourceId) {
        skippedSourceCount++;
        skippedSources.add(dep.source_unique_id);
        continue;
      }
      
      if (!targetId) {
        skippedTargetCount++;
        skippedTargets.add(dep.target_unique_id);
        continue;
      }

      if (sourceId && targetId) {
        await supabase
          .schema('metadata')
          .from('dependencies')
          .upsert({
            organization_id: organizationId,
            source_object_id: sourceId,
            target_object_id: targetId,
            dependency_type: 'dbt_ref',
            confidence: dep.confidence,
            metadata: {
              extracted_from: 'manifest',
              source_unique_id: dep.source_unique_id,
              target_unique_id: dep.target_unique_id
            }
          }, {
            onConflict: 'source_object_id,target_object_id,dependency_type,source_column,target_column'
          });

        storedCount++;

        // Build dependency map for lineage extraction
        if (!dependencyMap.has(dep.target_name)) {
          dependencyMap.set(dep.target_name, []);
        }
        dependencyMap.get(dep.target_name)!.push(dep.source_name);
      }
    }

    console.log(`‚úÖ Dependency storage complete:`);
    console.log(`   ‚úì Stored: ${storedCount}`);
    console.log(`   ‚ö†Ô∏è  Skipped (source not found): ${skippedSourceCount}`);
    console.log(`   ‚ö†Ô∏è  Skipped (target not found): ${skippedTargetCount}`);
    
    if (skippedSources.size > 0) {
      console.log(`\n   Missing source objects (first 10):`);
      Array.from(skippedSources).slice(0, 10).forEach(id => {
        console.log(`      - ${id}`);
      });
    }
    
    if (skippedTargets.size > 0) {
      console.log(`\n   Missing target objects (first 10):`);
      Array.from(skippedTargets).slice(0, 10).forEach(id => {
        console.log(`      - ${id}`);
      });
    }

    // Store column-level lineage
    await this.storeColumnLineage(
      connectionId,
      organizationId,
      parsed.columnLineage,
      parsed.models,
      objectMapByName,
      dependencyMap
    );

    console.log(`‚úÖ All data stored in database`);
  }

  /**
   * Extract and store column-level lineage
   * 
   * Strategy (Production-grade, like dbt Cloud/Atlan):
   * 1. Check if manifest has native column lineage (dbt 1.6+) - use if available
   * 2. Otherwise, parse compiled SQL using EnhancedSQLParser
   * 3. Use manifest dependencies as context for validation
   * 4. Store with confidence scores based on method
   */
  private async storeColumnLineage(
    connectionId: string,
    organizationId: string,
    manifestColumnLineages: any[],
    parsedModels: any[],
    objectMap: Map<string, string>,
    dependencyMap: Map<string, string[]>
  ): Promise<void> {
    console.log(`\n${'='.repeat(60)}`);
    console.log(`üîç COLUMN LINEAGE EXTRACTION`);
    console.log(`${'='.repeat(60)}\n`);

    let totalExtracted = 0;
    let totalStored = 0;
    let skipped = 0;

    // Check if manifest has native column lineage (dbt 1.6+)
    const hasManifestLineage = manifestColumnLineages && manifestColumnLineages.length > 0;
    
    if (hasManifestLineage) {
      console.log(`‚úÖ Manifest has native column lineage (dbt 1.6+)`);
      console.log(`   Using GOLD tier lineage from manifest\n`);
      
      // Use manifest lineage - highest accuracy
      const result = await this.storeManifestColumnLineage(
        organizationId,
        manifestColumnLineages,
        objectMap
      );
      totalExtracted += result.extracted;
      totalStored += result.stored;
      skipped += result.skipped;
    } else {
      console.log(`‚ÑπÔ∏è  No native column lineage in manifest (dbt < 1.6)`);
      console.log(`   Falling back to SQL parsing (SILVER/BRONZE tier)\n`);
    }

    // Parse compiled SQL for all models (even if we have manifest lineage)
    // This catches additional lineages that manifest might miss
    console.log(`üîç Parsing compiled SQL for additional column lineage...`);
    console.log(`   Using Python SQLGlot AST parser (95% accuracy)\n`);
    
    // Track stored columns to avoid duplicates
    const storedColumns = new Set<string>();
    
    // Check if Python service is available
    const pythonServiceAvailable = await this.pythonParser.healthCheck();
    
    if (!pythonServiceAvailable) {
      console.warn(`‚ö†Ô∏è  Python SQLGlot service not available - falling back to regex parser (70-80% accuracy)`);
      console.warn(`   To enable high-accuracy lineage, start the service: docker-compose up python-sqlglot-service`);
    }
    
    for (const model of parsedModels) {
      if (!model.compiled_sql || model.object_type !== 'model') {
        continue; // Skip seeds, sources, etc.
      }

      console.log(`\n   üìä Processing: ${model.name}`);

      try {
        // Get model dependencies from manifest
        const dependencies = dependencyMap.get(model.name) || [];
        
        if (dependencies.length === 0) {
          console.log(`      ‚ö†Ô∏è  No dependencies found - skipping`);
          continue;
        }

        console.log(`      Dependencies: ${dependencies.join(', ')}`);

        // Extract column lineage using Python SQLGlot AST parser (HIGH ACCURACY)
        // Falls back to EnhancedSQLParser if Python service unavailable
        let lineages: any[];
        
        if (pythonServiceAvailable) {
          // Use Python SQLGlot for 95% accuracy
          const pythonLineages = await this.pythonParser.extractColumnLineage(
            model.compiled_sql,
            model.name,
            { dialect: 'generic' }
          );
          
          // Transform Python lineage format to match EnhancedSQLParser format
          lineages = this.transformPythonLineageFormat(pythonLineages, model.name);
          console.log(`      üêç Python SQLGlot: ${pythonLineages.length} lineages (95% accuracy)`);
        } else {
          // Fallback to regex-based parser (70-80% accuracy)
          lineages = this.sqlParser.extractColumnLineage(
            model.compiled_sql,
            model.name,
            {
              dependencies: dependencies
            }
          );
          console.log(`      üìù Regex parser: ${lineages.length} lineages (70-80% accuracy)`);
        }

        totalExtracted += lineages.length;

        if (lineages.length === 0) {
          console.log(`      ‚ÑπÔ∏è  No column lineage extracted`);
          continue;
        }

        // Store each lineage relationship
        for (const lineage of lineages) {
          const targetObjectId = objectMap.get(model.name);
          if (!targetObjectId) {
            skipped++;
            continue;
          }

          for (const source of lineage.source_columns) {
            // Try multiple table name variants for better matching
            const tableVariants = this.getTableNameVariants(source.table);
            let sourceObjectId: string | undefined;
            
            // Try each variant until we find a match
            for (const variant of tableVariants) {
              sourceObjectId = objectMap.get(variant);
              if (sourceObjectId) {
                break;
              }
            }
            
            if (!sourceObjectId) {
              console.log(`      ‚ö†Ô∏è  Source table '${source.table}' (tried: [${tableVariants.join(', ')}]) not found in object map`);
              skipped++;
              continue;
            }

            // Store source and target columns if not already stored
            const sourceKey = `${sourceObjectId}:${source.column}`;
            const targetKey = `${targetObjectId}:${lineage.target_column}`;
            
            if (!storedColumns.has(sourceKey)) {
              await supabase
                .schema('metadata')
                .from('columns')
                .upsert({
                  object_id: sourceObjectId,
                  organization_id: organizationId,
                  name: source.column,
                  data_type: 'unknown',
                  is_nullable: true,
                  metadata: {
                    discovered_from: 'sql_parsing',
                    extracted_at: new Date().toISOString()
                  }
                }, {
                  onConflict: 'object_id,name'
                });
              storedColumns.add(sourceKey);
            }
            
            if (!storedColumns.has(targetKey)) {
              await supabase
                .schema('metadata')
                .from('columns')
                .upsert({
                  object_id: targetObjectId,
                  organization_id: organizationId,
                  name: lineage.target_column,
                  data_type: 'unknown',
                  is_nullable: true,
                  metadata: {
                    discovered_from: 'sql_parsing',
                    extracted_at: new Date().toISOString()
                  }
                }, {
                  onConflict: 'object_id,name'
                });
              storedColumns.add(targetKey);
            }

            // Store lineage relationship
            const { error } = await supabase
              .schema('metadata')
              .from('columns_lineage')
              .upsert({
                organization_id: organizationId,
                source_object_id: sourceObjectId,
                source_column: source.column,
                target_object_id: targetObjectId,
                target_column: lineage.target_column,
                transformation_type: source.transformation_type,
                confidence: source.confidence,
                extracted_from: pythonServiceAvailable ? 'python_sqlglot' : 'sql_parsing',
                expression: source.expression,
                metadata: {
                  parser: pythonServiceAvailable ? 'python-sqlglot-ast' : 'enhanced-sql-parser',
                  source_model: source.table,
                  target_model: model.name,
                  accuracy_tier: pythonServiceAvailable ? 'GOLD' : 'SILVER'
                }
              }, {
                onConflict: 'source_object_id,source_column,target_object_id,target_column'
              });

            if (error) {
              console.error(`      ‚ùå Error storing: ${source.table}.${source.column} ‚Üí ${lineage.target_column}`, error.message);
              skipped++;
            } else {
              console.log(`      ‚úÖ ${source.table}.${source.column} ‚Üí ${lineage.target_column} (${source.transformation_type}, ${(source.confidence * 100).toFixed(0)}%)`);
              totalStored++;
            }
          }
        }

      } catch (error: any) {
        console.error(`      ‚ùå Failed to parse ${model.name}:`, error?.message || error);
        skipped++;
      }
    }

    console.log(`\n${'='.repeat(60)}`);
    console.log(`üìä COLUMN LINEAGE SUMMARY`);
    console.log(`${'='.repeat(60)}`);
    console.log(`   Lineages Extracted: ${totalExtracted}`);
    console.log(`   Lineages Stored:    ${totalStored}`);
    console.log(`   Lineages Skipped:   ${skipped}`);
    console.log(`   Columns Discovered: ${storedColumns.size}`);
    console.log(`${'='.repeat(60)}\n`);
  }

  /**
   * Store manifest-based column lineage (dbt 1.6+)
   * GOLD tier - 100% accurate
   */
  private async storeManifestColumnLineage(
    organizationId: string,
    columnLineages: any[],
    objectMap: Map<string, string>
  ): Promise<{ extracted: number; stored: number; skipped: number }> {
    let stored = 0;
    let skipped = 0;

    console.log(`   Processing ${columnLineages.length} manifest column lineages...\n`);

    for (const lineage of columnLineages) {
      const targetObjectId = objectMap.get(lineage.target_model);
      if (!targetObjectId) {
        skipped++;
        continue;
      }

      for (const source of lineage.source_columns) {
        const sourceObjectId = objectMap.get(source.model);
        if (!sourceObjectId) {
          skipped++;
          continue;
        }

        const { error } = await supabase
          .schema('metadata')
          .from('columns_lineage')
          .upsert({
            organization_id: organizationId,
            source_object_id: sourceObjectId,
            source_column: source.column,
            target_object_id: targetObjectId,
            target_column: lineage.target_column,
            confidence: 1.0,  // GOLD tier - from manifest
            extracted_from: 'manifest',
            transformation_type: 'direct',
            metadata: {
              source: 'dbt_manifest',
              dbt_version: '1.6+',
              source_model: source.model,
              target_model: lineage.target_model
            }
          }, {
            onConflict: 'source_object_id,source_column,target_object_id,target_column'
          });

        if (!error) {
          console.log(`   ‚úÖ ${source.model}.${source.column} ‚Üí ${lineage.target_model}.${lineage.target_column} (manifest, 100%)`);
          stored++;
        } else {
          skipped++;
        }
      }
    }

    return {
      extracted: columnLineages.length,
      stored,
      skipped
    };
  }

  /**
   * Transform Python SQLGlot lineage format to match EnhancedSQLParser format
   * 
   * Python format:
   * { targetName: "stg_customers", sourceColumn: "id", targetColumn: "id", expression: "c.id" }
   * 
   * EnhancedSQLParser format:
   * { target_column: "id", source_columns: [{ table: "stg_customers", column: "id", ... }] }
   */
  private transformPythonLineageFormat(
    pythonLineages: Array<{
      targetName: string;
      sourceColumn: string;
      targetColumn: string;
      expression: string;
    }>,
    targetModel: string
  ): any[] {
    // Group by target column
    const grouped = new Map<string, Array<{
      table: string;
      column: string;
      expression: string;
    }>>();

    for (const lineage of pythonLineages) {
      if (!grouped.has(lineage.targetColumn)) {
        grouped.set(lineage.targetColumn, []);
      }
      
      grouped.get(lineage.targetColumn)!.push({
        table: lineage.targetName,
        column: lineage.sourceColumn,
        expression: lineage.expression
      });
    }

    // Convert to EnhancedSQLParser format
    const result: any[] = [];
    
    for (const [targetColumn, sources] of grouped.entries()) {
      result.push({
        target_column: targetColumn,
        source_columns: sources.map(src => ({
          table: src.table,
          column: src.column,
          transformation_type: this.classifyTransformation(src.expression),
          confidence: this.calculateConfidence(src.expression),
          expression: src.expression
        }))
      });
    }

    return result;
  }

  /**
   * Classify transformation type from SQL expression
   */
  private classifyTransformation(expression: string): string {
    const upper = expression.toUpperCase();

    if (/COUNT|SUM|AVG|MAX|MIN/i.test(upper)) {
      return 'aggregation';
    }
    if (/ROW_NUMBER|RANK|DENSE_RANK/i.test(upper)) {
      return 'window_function';
    }
    if (/CASE\s+WHEN/i.test(upper)) {
      return 'case_expression';
    }
    if (/CAST|CONVERT|::/i.test(expression)) {
      return 'cast';
    }
    if (/[+\-*\/]/.test(expression)) {
      return 'calculation';
    }
    if (/CONCAT|SUBSTRING|TRIM/i.test(upper)) {
      return 'string_function';
    }
    if (/COALESCE|NULLIF/i.test(upper)) {
      return 'null_handling';
    }
    
    return 'direct';
  }

  /**
   * Calculate confidence score based on transformation complexity
   */
  private calculateConfidence(expression: string): number {
    const transformationType = this.classifyTransformation(expression);
    
    const confidenceMap: Record<string, number> = {
      'direct': 0.95,
      'cast': 0.93,
      'aggregation': 0.90,
      'window_function': 0.88,
      'null_handling': 0.85,
      'string_function': 0.83,
      'calculation': 0.80,
      'case_expression': 0.75
    };

    return confidenceMap[transformationType] || 0.70;
  }

  /**
   * Clear all existing metadata for a repository before re-extraction
   * This prevents duplicate objects/columns on re-extraction
   * Matches IDE behavior exactly
   */
  private async clearRepositoryData(
    repositoryId: string,
    organizationId: string
  ): Promise<void> {
    console.log(`üßπ Clearing existing metadata for repository: ${repositoryId}`);
    console.log(`   Organization: ${organizationId}`);

    try {
      // Delete in reverse dependency order to maintain referential integrity
      
      // First, get all object IDs for this repository
      const { data: repoObjects } = await supabase
        .schema('metadata')
        .from('objects')
        .select('id')
        .eq('repository_id', repositoryId)
        .eq('organization_id', organizationId);
      
      const objectIds = repoObjects?.map(obj => obj.id) || [];
      
      if (objectIds.length === 0) {
        console.log('   ‚ÑπÔ∏è  No existing objects found - skipping cleanup');
        return;
      }
      
      console.log(`   üìä Found ${objectIds.length} objects to clean up`);
      
      // 1. Delete column lineage (references objects)
      const { error: lineageError } = await supabase
        .schema('metadata')
        .from('columns_lineage')
        .delete()
        .eq('organization_id', organizationId)
        .in('source_object_id', objectIds);
      
      if (lineageError) console.warn('   ‚ö†Ô∏è  Column lineage cleanup warning:', lineageError);
      else console.log('   ‚úÖ Cleared column lineage');

      // 2. Delete dependencies (references objects)
      const { error: depsError } = await supabase
        .schema('metadata')
        .from('dependencies')
        .delete()
        .eq('organization_id', organizationId)
        .in('source_object_id', objectIds);
      
      if (depsError) console.warn('   ‚ö†Ô∏è  Dependencies cleanup warning:', depsError);
      else console.log('   ‚úÖ Cleared dependencies');

      // 3. Delete columns (references objects)
      const { error: colsError } = await supabase
        .schema('metadata')
        .from('columns')
        .delete()
        .eq('organization_id', organizationId)
        .in('object_id', objectIds);
      
      if (colsError) console.warn('   ‚ö†Ô∏è  Columns cleanup warning:', colsError);
      else console.log('   ‚úÖ Cleared columns');

      // 4. Delete objects
      const { error: objsError } = await supabase
        .schema('metadata')
        .from('objects')
        .delete()
        .eq('repository_id', repositoryId)
        .eq('organization_id', organizationId);
      
      if (objsError) console.warn('   ‚ö†Ô∏è  Objects cleanup warning:', objsError);
      else console.log('   ‚úÖ Cleared objects');

      // 5. Delete files
      const { error: filesError } = await supabase
        .schema('metadata')
        .from('files')
        .delete()
        .eq('repository_id', repositoryId)
        .eq('organization_id', organizationId);
      
      if (filesError) console.warn('   ‚ö†Ô∏è  Files cleanup warning:', filesError);
      else console.log('   ‚úÖ Cleared files');

      console.log(`‚úÖ Repository cleanup completed successfully`);
    } catch (error: any) {
      console.error(`‚ùå Error during repository cleanup:`, error);
      // Don't throw - allow extraction to proceed with fresh insert
      // This matches IDE behavior of graceful degradation
    }
  }
}
