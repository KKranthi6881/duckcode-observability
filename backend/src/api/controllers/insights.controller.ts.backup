import { NextFunction, Response, Request } from 'express';
import { SupabaseClient } from '@supabase/supabase-js';
import supabaseInstance, { supabaseCodeInsights } from '@/config/supabaseClient';
import * as githubService from '@/services/github.service';
import * as aiService from '@/services/ai.service';

console.log('[InsightsController] supabaseInstance imported:', typeof supabaseInstance, supabaseInstance !== null, supabaseInstance ? Object.keys(supabaseInstance) : 'null or undefined');

// Correctly type supabase to match the instance from supabaseClient.ts
const supabase: SupabaseClient<any, 'code_insights'> = supabaseCodeInsights;

// A list of file extensions to process. Add or remove as needed.
// An empty list would mean processing all files.
const ALLOWED_EXTENSIONS = [
  '.sql', '.py', '.md', 
  '.java', '.js', '.ts', '.tsx', '.jsx',
  '.yaml', '.yml', '.json',
  '.ipynb' // Jupyter Notebooks
];

// Enhanced function to determine the language from a file path with intelligent SQL dialect detection
const getLanguageFromFilePath = (filePath: string): string => {
  const extension = filePath.split('.').pop()?.toLowerCase();
  const lowerPath = filePath.toLowerCase();
  
  // Enhanced SQL dialect detection
  if (extension === 'sql') {
    // dbt-specific patterns
    if (lowerPath.includes('dbt') || lowerPath.includes('models/') || 
        lowerPath.includes('staging/') || lowerPath.includes('marts/') ||
        lowerPath.includes('intermediate/') || lowerPath.includes('macros/') ||
        lowerPath.includes('snapshots/') || lowerPath.includes('analyses/') ||
        lowerPath.includes('seeds/') || lowerPath.includes('/dbt_') ||
        lowerPath.includes('_housekeeping')) {
      return 'dbt';
    }
    
    // Database-specific patterns
    if (lowerPath.includes('postgres') || lowerPath.includes('postgresql')) return 'postgres';
    if (lowerPath.includes('mysql')) return 'mysql';
    if (lowerPath.includes('oracle') || lowerPath.includes('plsql')) return 'plsql';
    if (lowerPath.includes('sqlserver') || lowerPath.includes('tsql')) return 'tsql';
    if (lowerPath.includes('snowflake')) return 'snowflake';
    if (lowerPath.includes('bigquery')) return 'bigquery';
    if (lowerPath.includes('redshift')) return 'redshift';
    
    // Default to postgres for generic SQL (most comprehensive SQL dialect)
    return 'postgres';
  }
  
  // Python variants
  if (extension === 'py') {
    if (lowerPath.includes('pyspark') || lowerPath.includes('spark')) return 'pyspark';
    return 'python';
  }
  
  // Other file types
  switch (extension) {
    case 'ipynb': return 'python_notebook';
    case 'md': return 'markdown';
    case 'js': return 'javascript';
    case 'ts': return 'typescript';
    case 'java': return 'java';
    case 'r': return 'r';
    case 'scala': return 'scala';
    case 'yml':
    case 'yaml': 
      // dbt YAML files
      if (lowerPath.includes('dbt') || lowerPath.includes('models/')) return 'dbt';
      return 'yaml';
    default: return extension || 'unknown';
  }
};

/**
 * @description Initiates the processing of a GitHub repository.
 * It fetches all files from the repo, filters them, and adds them to the
 * `code_insights.files` table to be picked up by processing workers.
 * @route POST /api/insights/process-repository
 */
export const processRepository = async (req: Request, res: Response, next: NextFunction) => {
  // ⚠️ OLD PARALLEL PROCESSING DISABLED ⚠️
  // This endpoint has been replaced by sequential processing
  // Use /api/sequential/start instead
  
  return res.status(410).json({
    error: 'This endpoint has been deprecated',
    message: 'Old parallel processing has been disabled. Please use the new sequential processing endpoint: /api/sequential/start',
    newEndpoint: '/api/sequential/start',
    documentation: 'The new sequential processing provides better control and monitoring of the 5-phase pipeline.'
  });
};

export const getRepositoryProcessingStatus = async (req: Request, res: Response, next: NextFunction) => {
  try {
    const userId = req.user?.id;
    if (!userId) {
      return res.status(401).json({ error: 'User not authenticated' });
    }

    const { repositoryFullName } = req.params;
    if (!repositoryFullName) {
      return res.status(400).json({ message: 'A valid repositoryFullName is required.' });
    }

    console.log(`[InsightsController] Fetching comprehensive processing status for repository: ${repositoryFullName}`);

    // Use the new comprehensive status function that includes vector processing
    const { data: statusData, error: statusError } = await supabaseCodeInsights
      .rpc('get_repository_processing_status', {
        repo_full_name: repositoryFullName,
        user_id_param: userId
      });

    if (statusError) {
      console.error('Supabase DB Error during comprehensive status fetch:', statusError);
      throw new Error('Failed to fetch processing status.');
    }

    if (!statusData || statusData.length === 0) {
      return res.status(404).json({ message: 'No files found for this repository. Has it been processed?' });
    }

    const status = statusData[0];
    
    // Parse the file_details JSONB into a more usable format
    const detailedStatus = status.file_details.map((file: any) => ({
      filePath: file.filePath,
      documentationStatus: file.docStatus,
      vectorStatus: file.vectorStatus,
      overallStatus: file.overallStatus,
      documentationError: file.docError,
      vectorError: file.vectorError,
      vectorChunks: file.vectorChunks || 0,
      isFullyProcessed: file.overallStatus === 'completed'
    }));

    const response = {
      // Overall metrics
      totalFiles: Number(status.total_files),
      overallCompleted: Number(status.overall_completed),
      overallProgress: Number(status.overall_progress),
      
      // Documentation-specific metrics
      documentation: {
        completed: Number(status.documentation_completed),
        failed: Number(status.documentation_failed),
        pending: Number(status.documentation_pending),
        progress: status.total_files > 0 ? 
          Math.round((Number(status.documentation_completed) / Number(status.total_files)) * 100) : 0
      },
      
      // Vector-specific metrics
      vectors: {
        completed: Number(status.vector_completed),
        failed: Number(status.vector_failed),
        pending: Number(status.vector_pending),
        progress: status.total_files > 0 ? 
          Math.round((Number(status.vector_completed) / Number(status.total_files)) * 100) : 0
      },
      
      // Detailed file-by-file status
      detailedStatus,
      
      // Legacy compatibility (for existing frontend)
      completed: Number(status.overall_completed),
      failed: Number(status.documentation_failed) + Number(status.vector_failed),
      pending: Number(status.documentation_pending) + Number(status.vector_pending),
      progress: Number(status.overall_progress)
    };

    console.log(`[InsightsController] Comprehensive status response:`, {
      totalFiles: response.totalFiles,
      overallProgress: response.overallProgress,
      docProgress: response.documentation.progress,
      vectorProgress: response.vectors.progress
    });

    res.status(200).json(response);

  } catch (error) {
    console.error(`[InsightsController] Error in getRepositoryProcessingStatus:`, error);
    next(error);
  }
};

/**
 * @description Retry vector processing for failed files
 * @route POST /api/insights/retry-vectors/:repositoryFullName
 */
export const retryVectorProcessing = async (req: Request, res: Response, next: NextFunction) => {
  try {
    const userId = req.user?.id;
    if (!userId) {
      return res.status(401).json({ error: 'User not authenticated' });
    }

    const { repositoryFullName } = req.params;
    if (!repositoryFullName) {
      return res.status(400).json({ message: 'A valid repositoryFullName is required.' });
    }

    console.log(`[InsightsController] Retrying vector processing for repository: ${repositoryFullName}`);

    // Find files with failed vector processing or completed documentation but pending vectors
    const { data: jobsToRetry, error: jobsError } = await supabaseCodeInsights
      .from('processing_jobs')
      .select(`
        id,
        file_id,
        status,
        vector_status,
        files!inner(repository_full_name, file_path)
      `)
      .eq('files.repository_full_name', repositoryFullName)
      .eq('files.user_id', userId)
      .eq('status', 'completed') // Only retry vector processing for files with completed documentation
      .in('vector_status', ['failed', 'pending']);

    if (jobsError) {
      console.error('Error finding jobs to retry:', jobsError);
      throw new Error('Failed to find vector processing jobs to retry');
    }

    if (!jobsToRetry || jobsToRetry.length === 0) {
      return res.status(200).json({
        message: 'No vector processing jobs found that need retry',
        retriedCount: 0
      });
    }

    // Reset vector processing status to pending for retry
    const jobIds = jobsToRetry.map(job => job.id);
    
    const { error: resetError } = await supabaseCodeInsights
      .from('processing_jobs')
      .update({
        vector_status: 'pending',
        vector_error_details: null,
        vector_processed_at: null,
        updated_at: new Date().toISOString()
      })
      .in('id', jobIds);

    if (resetError) {
      console.error('Error resetting vector processing status:', resetError);
      throw new Error('Failed to reset vector processing status');
    }

    console.log(`[InsightsController] Reset vector processing status for ${jobIds.length} jobs`);

    // Note: The actual vector processing will be picked up by the next run of the code processor
    // You might want to trigger the code processor function here if you have a way to do so

    res.status(200).json({
      message: `Successfully reset vector processing for ${jobIds.length} files. Processing will resume automatically.`,
      retriedCount: jobIds.length,
      jobIds
    });

  } catch (error) {
    console.error(`[InsightsController] Error in retryVectorProcessing:`, error);
    next(error);
  }
};

/**
 * @description Fetches the code summary for a specific file from code_insights.code_summaries table
 * @route GET /api/insights/file-summary/:repositoryFullName/:filePath
 */
export const getFileSummary = async (req: Request, res: Response, next: NextFunction) => {
  try {
    const userId = req.user?.id;
    if (!userId) {
      return res.status(401).json({ error: 'User not authenticated' });
    }

    const { repositoryFullName } = req.params;
    const filePath = req.params[0]; // Using wildcard parameter to capture full file path with slashes

    console.log(`[InsightsController] === FILE SUMMARY REQUEST DEBUG ===`);
    console.log(`[InsightsController] Raw request params:`, req.params);
    console.log(`[InsightsController] Repository: ${repositoryFullName}`);
    console.log(`[InsightsController] File path: ${filePath}`);
    console.log(`[InsightsController] User ID: ${userId}`);
    console.log(`[InsightsController] Request URL: ${req.originalUrl}`);

    if (!repositoryFullName || !filePath) {
      return res.status(400).json({ message: 'Repository name and file path are required.' });
    }

    // First, let's see what files exist in the database for this repository
    const { data: allRepoFiles, error: allRepoFilesError } = await supabaseCodeInsights
      .from('files')
      .select('id, file_path, language, repository_full_name')
      .eq('repository_full_name', repositoryFullName)
      .eq('user_id', userId);

    console.log(`[InsightsController] All files in repository ${repositoryFullName}:`, 
      allRepoFiles?.map(f => ({ id: f.id, path: f.file_path, lang: f.language })));

    // Check for exact match
    const exactMatches = allRepoFiles?.filter(f => f.file_path === filePath);
    console.log(`[InsightsController] Exact path matches for "${filePath}":`, exactMatches);

    // Check for similar matches (case-insensitive, etc.)
    const similarMatches = allRepoFiles?.filter(f => 
      f.file_path.toLowerCase().includes(filePath.toLowerCase()) ||
      filePath.toLowerCase().includes(f.file_path.toLowerCase())
    );
    console.log(`[InsightsController] Similar path matches:`, similarMatches);

    // First, get the file record to get the file_id
    const { data: fileRecord, error: fileError } = await supabaseCodeInsights
      .from('files')
      .select('id, file_path, language, last_processed_at')
      .eq('repository_full_name', repositoryFullName)
      .eq('file_path', filePath)
      .eq('user_id', userId)
      .single();

    console.log(`[InsightsController] Database query result:`, { fileRecord, fileError });

    if (fileError || !fileRecord) {
      console.log(`[InsightsController] File not found: ${filePath} in ${repositoryFullName}`);
      console.log(`[InsightsController] File error details:`, fileError);
      
      return res.status(404).json({ 
        message: 'File not found or not processed yet.',
        debug: {
          requestedPath: filePath,
          repository: repositoryFullName,
          availableFiles: allRepoFiles?.map(f => f.file_path).slice(0, 10)
        }
      });
    }

    // Now get the code summary for this file
    const { data: summaryRecord, error: summaryError } = await supabaseCodeInsights
      .from('code_summaries')
      .select('summary_json, created_at')
      .eq('file_id', fileRecord.id)
      .order('created_at', { ascending: false })
      .limit(1)
      .single();

    console.log(`[InsightsController] Summary query result:`, { summaryRecord, summaryError });

    if (summaryError || !summaryRecord) {
      console.log(`[InsightsController] No summary found for file_id: ${fileRecord.id}`);
      
      // Let's also check what summaries DO exist for this repository to help debug
      const { data: allSummaries, error: allSummariesError } = await supabaseCodeInsights
        .from('code_summaries')
        .select('file_id, created_at')
        .in('file_id', 
          await supabaseCodeInsights
            .from('files')
            .select('id')
            .eq('repository_full_name', repositoryFullName)
            .eq('user_id', userId)
            .then(result => result.data?.map(f => f.id) || [])
        )
        .limit(10);
      
      console.log(`[InsightsController] Available summaries for repository files:`, allSummaries);
      
      return res.status(404).json({ 
        message: 'Code summary not available for this file.',
        fileInfo: {
          filePath: fileRecord.file_path,
          language: fileRecord.language,
          lastProcessed: fileRecord.last_processed_at
        }
      });
    }

    // Extract the actual summary content from the OpenAI response structure
    let extractedSummary = summaryRecord.summary_json;
    
    console.log(`[InsightsController] Raw summary_json:`, summaryRecord.summary_json);
    
    // Check if this is an OpenAI API response structure
    if (summaryRecord.summary_json?.choices?.[0]?.message?.content) {
      console.log(`[InsightsController] Extracting content from OpenAI response structure`);
      const contentString = summaryRecord.summary_json.choices[0].message.content;
      
      try {
        // Try to parse the content as JSON (in case it's structured)
        extractedSummary = JSON.parse(contentString);
        console.log(`[InsightsController] Parsed content as JSON:`, extractedSummary);
      } catch {
        // If it's not JSON, use the content string directly
        extractedSummary = { summary: contentString };
        console.log(`[InsightsController] Using content as plain text`);
      }
    } else {
      console.log(`[InsightsController] Using summary_json as-is`);
    }

    return res.status(200).json({
      filePath: fileRecord.file_path,
      language: fileRecord.language,
      lastProcessed: fileRecord.last_processed_at,
      summary: extractedSummary,
      summaryCreatedAt: summaryRecord.created_at
    });

  } catch (error) {
    console.error(`[InsightsController] Error in getFileSummary:`, error);
    next(error);
  }
};

export const generateRepositorySummaries = async (req: Request, res: Response, next: NextFunction) => {
  try {
    const { owner, repo } = req.params;
    const { selectedLanguage } = req.body; // Get selected language for specialized analysis
    const repositoryFullName = `${owner}/${repo}`;
    const userId = (req as any).user?.id;

    if (!userId) {
      return res.status(401).json({ message: 'User not authenticated' });
    }

    console.log(`[InsightsController] Starting AI summary generation for repository: ${repositoryFullName}`);
    if (selectedLanguage) {
      console.log(`[InsightsController] Using specialized analysis for language: ${selectedLanguage}`);
    }

    // Check if user has access to this repository and get Octokit instance
    const octokit = await githubService.getOctokitForUser(userId);
    if (!octokit) {
      return res.status(404).json({ message: 'GitHub App installation not found for user' });
    }

    // Get files that don't have summaries yet
    const { data: filesWithoutSummaries, error: filesError } = await supabaseCodeInsights
      .from('files')
      .select(`
        id,
        file_path,
        language,
        size_bytes
      `)
      .eq('repository_full_name', repositoryFullName)
      .eq('user_id', userId)
      .not('id', 'in', `(
        SELECT file_id 
        FROM code_summaries 
        WHERE file_id IS NOT NULL
      )`);

    if (filesError) {
      console.error(`[InsightsController] Error fetching files without summaries:`, filesError);
      return res.status(500).json({ message: 'Failed to fetch files for summary generation' });
    }

    if (!filesWithoutSummaries || filesWithoutSummaries.length === 0) {
      return res.status(200).json({ 
        message: 'All files already have summaries',
        processed: 0,
        skipped: 0
      });
    }

    console.log(`[InsightsController] Found ${filesWithoutSummaries.length} files without summaries`);

    // Check if AI service is ready
    if (!aiService.isAIServiceReady()) {
      return res.status(503).json({ 
        message: 'AI service not available. Please ensure OPENAI_API_KEY is configured.' 
      });
    }

    let processed = 0;
    let failed = 0;
    const batchSize = 5; // Process files in small batches to avoid rate limits

    // Process files in batches
    for (let i = 0; i < filesWithoutSummaries.length; i += batchSize) {
      const batch = filesWithoutSummaries.slice(i, i + batchSize);
      
      await Promise.all(batch.map(async (file) => {
        try {
          console.log(`[InsightsController] Generating summary for file: ${file.file_path}`);

          // Get file content from GitHub
          const fileContent = await aiService.getFileContentFromGitHub(
            octokit,
            owner,
            repo,
            file.file_path
          );

          if (!fileContent) {
            console.log(`[InsightsController] Skipping file (no content): ${file.file_path}`);
            return;
          }

          // Generate AI summary with selected language for specialized analysis
          const summary = await aiService.generateCodeSummary(
            fileContent, 
            file.file_path, 
            file.language || 'unknown',
            selectedLanguage // Pass selected language for specialized prompts
          );

          // Save summary to database
          const { error: insertError } = await supabaseCodeInsights
            .from('code_summaries')
            .insert({
              file_id: file.id,
              summary_json: summary,
              created_at: new Date().toISOString(),
              updated_at: new Date().toISOString()
            });

          if (insertError) {
            console.error(`[InsightsController] Error saving summary for ${file.file_path}:`, insertError);
            failed++;
          } else {
            console.log(`[InsightsController] Successfully generated summary for: ${file.file_path}`);
            processed++;
          }

        } catch (error) {
          console.error(`[InsightsController] Error processing file ${file.file_path}:`, error);
          failed++;
        }
      }));

      // Small delay between batches to respect rate limits
      if (i + batchSize < filesWithoutSummaries.length) {
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }

    console.log(`[InsightsController] Summary generation completed. Processed: ${processed}, Failed: ${failed}`);

    res.status(200).json({
      message: `Summary generation completed`,
      processed,
      failed,
      total: filesWithoutSummaries.length
    });

  } catch (error) {
    console.error(`[InsightsController] Error in generateRepositorySummaries:`, error);
    next(error);
  }
};

/**
 * @description Initiates ONLY documentation processing for sequential pipeline
 * @route POST /api/insights/process-documentation-only
 */
export const processDocumentationOnly = async (req: Request, res: Response, next: NextFunction) => {
  // ⚠️ DEPRECATED - Use /api/phases/documentation instead
  return res.status(410).json({
    error: 'This endpoint has been deprecated',
    message: 'Please use the new phase-specific endpoint: /api/phases/documentation',
    newEndpoint: '/api/phases/documentation',
    documentation: 'The new processing phases provide cleaner separation and better error handling.'
  });
};

/**
 * @description Initiates ONLY vector processing for sequential pipeline
 * @route POST /api/insights/process-vectors-only
 */
export const processVectorsOnly = async (req: Request, res: Response, next: NextFunction) => {
  // ⚠️ DEPRECATED - Use /api/phases/vectors instead
  return res.status(410).json({
    error: 'This endpoint has been deprecated',
    message: 'Please use the new phase-specific endpoint: /api/phases/vectors',
    newEndpoint: '/api/phases/vectors',
    documentation: 'The new processing phases provide cleaner separation and better error handling.'
  });
};

/**
 * @description Initiates ONLY lineage processing for sequential pipeline
 * @route POST /api/insights/process-lineage-only
 */
export const processLineageOnly = async (req: Request, res: Response, next: NextFunction) => {
  // ⚠️ DEPRECATED - Use /api/phases/lineage instead
  return res.status(410).json({
    error: 'This endpoint has been deprecated',
    message: 'Please use the new phase-specific endpoint: /api/phases/lineage',
    newEndpoint: '/api/phases/lineage',
    documentation: 'The new processing phases provide cleaner separation and better error handling.'
  });
};
  try {
    const userId = req.user?.id;
    if (!userId) {
      return res.status(401).json({ error: 'User not authenticated' });
    }

    const { repositoryFullName } = req.body;
    if (!repositoryFullName || !repositoryFullName.includes('/')) {
      return res.status(400).json({ message: 'A valid repositoryFullName (e.g., "owner/repo") is required.' });
    }

    console.log(`[InsightsController] Starting LINEAGE-ONLY processing for repository: ${repositoryFullName}`);

    // Find SQL files with completed vector processing but no lineage processing
    const { data: completedVectors, error: findError } = await supabaseCodeInsights
      .from('processing_jobs')
      .select(`
        id,
        file_id,
        status,
        vector_status,
        lineage_status,
        files!inner(repository_full_name, file_path, language, user_id)
      `)
      .eq('files.repository_full_name', repositoryFullName)
      .eq('files.user_id', userId)
      .eq('status', 'completed') // Documentation must be completed
      .eq('vector_status', 'completed') // Vectors must be completed
      .is('lineage_status', null); // No lineage processing yet

    if (findError) {
      console.error('Error finding files ready for lineage processing:', findError);
      throw new Error(`Failed to find files ready for lineage processing: ${findError.message}`);
    }

    if (!completedVectors || completedVectors.length === 0) {
      return res.status(200).json({
        message: 'No files found with completed vectors ready for lineage processing',
        filesQueued: 0,
        phase: 'lineage'
      });
    }

    // Filter for SQL-eligible files and update them for lineage processing
    const sqlFiles = completedVectors.filter(job => {
      const language = (job as any).files.language?.toLowerCase() || '';
      const filePath = (job as any).files.file_path?.toLowerCase() || '';
      return language.includes('sql') || 
             language.includes('postgres') ||
             language.includes('mysql') ||
             language.includes('snowflake') ||
             language.includes('bigquery') ||
             language.includes('redshift') ||
             language.includes('dbt') ||
             filePath.endsWith('.sql');
    });

    if (sqlFiles.length === 0) {
      return res.status(200).json({
        message: 'No SQL files found ready for lineage processing',
        filesQueued: 0,
        phase: 'lineage'
      });
    }

    // Update jobs to enable lineage processing
    const jobIds = sqlFiles.map(job => job.id);
    
    const { error: updateError } = await supabaseCodeInsights
      .from('processing_jobs')
      .update({
        lineage_status: 'pending',
        updated_at: new Date().toISOString()
      })
      .in('id', jobIds);

    if (updateError) {
      console.error('Error updating jobs for lineage processing:', updateError);
      throw new Error(`Failed to enable lineage processing: ${updateError.message}`);
    }

    console.log(`Successfully enabled lineage processing for ${sqlFiles.length} SQL files.`);

    res.status(202).json({
      message: `Lineage processing initiated for ${sqlFiles.length} SQL files with completed vectors.`,
      filesQueued: sqlFiles.length,
      totalFilesChecked: completedVectors.length,
      phase: 'lineage'
    });

  } catch (error: any) {
    console.error(`[InsightsController] Error in processLineageOnly:`, error);
    next(error);
  }
};

/**
 * @description Initiates ONLY dependencies processing for sequential pipeline (Phase 4)
 * @route POST /api/insights/process-dependencies-only
 */
export const processDependenciesOnly = async (req: Request, res: Response, next: NextFunction) => {
  try {
    const userId = req.user?.id;
    if (!userId) {
      return res.status(401).json({ error: 'User not authenticated' });
    }

    const { repositoryFullName } = req.body;
    if (!repositoryFullName || !repositoryFullName.includes('/')) {
      return res.status(400).json({ message: 'A valid repositoryFullName (e.g., "owner/repo") is required.' });
    }

    console.log(`[InsightsController] Starting DEPENDENCIES-ONLY processing for repository: ${repositoryFullName}`);

    // Find files with completed lineage processing ready for dependency analysis
    const { data: completedLineage, error: findError } = await supabaseCodeInsights
      .from('processing_jobs')
      .select(`
        id,
        file_id,
        status,
        vector_status,
        lineage_status,
        files!inner(repository_full_name, file_path, language, user_id)
      `)
      .eq('files.repository_full_name', repositoryFullName)
      .eq('files.user_id', userId)
      .eq('status', 'completed') // Documentation must be completed
      .eq('vector_status', 'completed') // Vectors must be completed
      .in('lineage_status', ['completed', 'null']) // Lineage completed or not applicable
      .order('created_at', { ascending: true });

    if (findError) {
      console.error('Error finding files ready for dependency processing:', findError);
      throw new Error(`Failed to find files ready for dependency processing: ${findError.message}`);
    }

    if (!completedLineage || completedLineage.length === 0) {
      return res.status(200).json({
        message: 'No files found ready for dependency processing',
        filesQueued: 0,
        phase: 'dependencies'
      });
    }

    // Process cross-file dependencies using existing lineage data
    console.log(`Processing dependencies for ${completedLineage.length} files`);

    try {
      // Get all asset relationships from lineage data
      const { data: assetRelationships, error: assetsError } = await supabaseCodeInsights
        .from('asset_relationships')
        .select(`
          id,
          source_asset_id,
          target_asset_id,
          relationship_type,
          transformation_logic,
          confidence_score,
          source_assets!source_asset_id(name, type, schema_name, database_name),
          target_assets!target_asset_id(name, type, schema_name, database_name)
        `)
        .eq('repository_full_name', repositoryFullName);

      // Get all file dependencies from lineage data  
      const { data: fileDependencies, error: depsError } = await supabaseCodeInsights
        .from('file_dependencies')
        .select('*')
        .eq('repository_full_name', repositoryFullName);

      if (!assetsError && !depsError) {
        // Build comprehensive dependency graph
        const dependencyGraph = {
          nodes: [] as any[],
          edges: [] as any[],
          crossFileConnections: [] as any[],
          columnLineage: [] as any[],
          businessImpactMap: [] as any[]
        };

        // Process asset relationships into dependency graph
        if (assetRelationships) {
          for (const relationship of assetRelationships) {
            const sourceAsset = (relationship as any).source_assets;
            const targetAsset = (relationship as any).target_assets;
            
            if (sourceAsset && targetAsset) {
              dependencyGraph.crossFileConnections.push({
                source: `${sourceAsset.database_name || 'default'}.${sourceAsset.schema_name || 'default'}.${sourceAsset.name}`,
                target: `${targetAsset.database_name || 'default'}.${targetAsset.schema_name || 'default'}.${targetAsset.name}`,
                type: relationship.relationship_type,
                confidence: relationship.confidence_score,
                transformationLogic: relationship.transformation_logic
              });
            }
          }
        }

        // Process file dependencies
        if (fileDependencies) {
          for (const fileDep of fileDependencies) {
            dependencyGraph.nodes.push({
              id: fileDep.file_id,
              name: fileDep.import_path,
              type: fileDep.import_type,
              confidence: fileDep.confidence_score
            });
          }
        }

        // Store enhanced dependency analysis
        const { error: storeError } = await supabaseCodeInsights
          .from('repository_dependency_analysis')
          .upsert({
            repository_full_name: repositoryFullName,
            user_id: userId,
            dependency_graph: dependencyGraph,
            analysis_type: 'cross_file_dependencies',
            created_at: new Date().toISOString(),
            updated_at: new Date().toISOString()
          }, {
            onConflict: 'repository_full_name,user_id,analysis_type'
          });

        if (storeError) {
          console.error('Error storing dependency analysis:', storeError);
        } else {
          console.log(`✅ Dependencies analysis completed for ${repositoryFullName}`);
        }
      }

    } catch (processingError) {
      console.error('Error processing dependencies:', processingError);
    }

    res.status(202).json({
      message: `Dependencies processing completed for ${completedLineage.length} files.`,
      filesQueued: completedLineage.length,
      phase: 'dependencies'
    });

  } catch (error: any) {
    console.error(`[InsightsController] Error in processDependenciesOnly:`, error);
    next(error);
  }
};

/**
 * @description Initiates ONLY analysis processing for sequential pipeline (Phase 5)
 * @route POST /api/insights/process-analysis-only
 */
export const processAnalysisOnly = async (req: Request, res: Response, next: NextFunction) => {
  try {
    const userId = req.user?.id;
    if (!userId) {
      return res.status(401).json({ error: 'User not authenticated' });
    }

    const { repositoryFullName } = req.body;
    if (!repositoryFullName || !repositoryFullName.includes('/')) {
      return res.status(400).json({ message: 'A valid repositoryFullName (e.g., "owner/repo") is required.' });
    }

    console.log(`[InsightsController] Starting ANALYSIS-ONLY processing for repository: ${repositoryFullName}`);

    // Get comprehensive data for impact analysis
    const { data: summaries, error: summariesError } = await supabaseCodeInsights
      .from('code_summaries')
      .select(`
        id,
        file_id,
        summary_json,
        files!inner(repository_full_name, file_path, language, user_id)
      `)
      .eq('files.repository_full_name', repositoryFullName)
      .eq('files.user_id', userId);

    if (summariesError || !summaries || summaries.length === 0) {
      return res.status(400).json({
        message: 'No file summaries found for impact analysis',
        phase: 'analysis'
      });
    }

    try {
      // Analyze business impact and technical complexity
      let totalComplexity = 0;
      let businessCriticalFiles = 0;
      const kpiMetrics = new Set();
      const businessDomains = new Set();
      const riskFactors = [];

      for (const summary of summaries) {
        const summaryData = summary.summary_json;
        
        // Extract business metrics
        if (summaryData?.business_logic?.kpis_metrics) {
          summaryData.business_logic.kpis_metrics.forEach((kpi: string) => kpiMetrics.add(kpi));
        }
        
        // Extract business domains
        if (summaryData?.business_logic?.data_domain) {
          businessDomains.add(summaryData.business_logic.data_domain);
        }
        
        // Assess criticality
        if (summaryData?.business_logic?.business_criticality === 'high' || 
            summaryData?.summary?.complexity === 'Complex') {
          businessCriticalFiles++;
        }
        
        // Calculate complexity score
        if (summaryData?.summary?.complexity) {
          const complexityScore = summaryData.summary.complexity === 'Complex' ? 3 : 
                                 summaryData.summary.complexity === 'Moderate' ? 2 : 1;
          totalComplexity += complexityScore;
        }
        
        // Identify risk factors
        if (summaryData?.performance_considerations?.resource_usage) {
          riskFactors.push({
            file: (summary as any).files.file_path,
            risks: summaryData.performance_considerations.resource_usage
          });
        }
      }

      // Create comprehensive impact analysis
      const impactAnalysis = {
        repository_summary: {
          total_files: summaries.length,
          business_critical_files: businessCriticalFiles,
          average_complexity: totalComplexity / summaries.length,
          business_domains: Array.from(businessDomains),
          key_metrics: Array.from(kpiMetrics)
        },
        risk_assessment: {
          high_risk_files: riskFactors.filter(r => r.risks.length > 2).length,
          performance_concerns: riskFactors.length,
          complexity_distribution: {
            high: summaries.filter(s => s.summary_json?.summary?.complexity === 'Complex').length,
            medium: summaries.filter(s => s.summary_json?.summary?.complexity === 'Moderate').length,
            low: summaries.filter(s => s.summary_json?.summary?.complexity === 'Simple').length
          }
        },
        recommendations: [
          businessCriticalFiles > summaries.length * 0.5 ? 
            'High proportion of business-critical files detected. Consider enhanced monitoring.' : 
            'Business criticality distribution appears balanced.',
          totalComplexity / summaries.length > 2.5 ? 
            'High average complexity detected. Consider refactoring opportunities.' : 
            'Complexity levels appear manageable.',
          riskFactors.length > summaries.length * 0.3 ? 
            'Performance concerns identified in multiple files. Review resource usage.' : 
            'Performance risk appears low.'
        ]
      };

      // Store impact analysis
      const { error: storeError } = await supabaseCodeInsights
        .from('repository_dependency_analysis')
        .upsert({
          repository_full_name: repositoryFullName,
          user_id: userId,
          dependency_graph: impactAnalysis,
          analysis_type: 'impact_analysis',
          created_at: new Date().toISOString(),
          updated_at: new Date().toISOString()
        }, {
          onConflict: 'repository_full_name,user_id,analysis_type'
        });

      if (storeError) {
        console.error('Error storing impact analysis:', storeError);
      } else {
        console.log(`✅ Impact analysis completed for ${repositoryFullName}`);
      }

    } catch (analysisError) {
      console.error('Error processing impact analysis:', analysisError);
    }

    res.status(202).json({
      message: `Impact analysis completed for ${summaries.length} files.`,
      filesQueued: summaries.length,
      phase: 'analysis'
    });

  } catch (error: any) {
    console.error(`[InsightsController] Error in processAnalysisOnly:`, error);
    next(error);
  }
};
