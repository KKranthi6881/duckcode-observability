-- ============================================
-- PHASE 2: Smart Recommendations - SQL Queries
-- Test these queries in your Snowflake instance BEFORE implementation
-- ============================================

-- ============================================
-- 1. WAREHOUSE UTILIZATION & LOAD HISTORY
-- Purpose: Analyze warehouse utilization for right-sizing recommendations
-- ============================================

-- Query 1A: Warehouse Load History (Last 30 days)
SELECT 
  WAREHOUSE_NAME,
  START_TIME,
  END_TIME,
  AVG_RUNNING,
  AVG_QUEUED_LOAD,
  AVG_QUEUED_PROVISIONING,
  AVG_BLOCKED
FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_LOAD_HISTORY
WHERE START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
ORDER BY START_TIME DESC
LIMIT 100;

-- Expected columns:
-- WAREHOUSE_NAME, START_TIME, END_TIME, AVG_RUNNING, AVG_QUEUED_LOAD, 
-- AVG_QUEUED_PROVISIONING, AVG_BLOCKED


-- Query 1B: Warehouse Utilization Summary (for right-sizing)
WITH warehouse_stats AS (
  SELECT 
    wh.WAREHOUSE_NAME,
    wh.WAREHOUSE_SIZE,
    AVG(wlh.AVG_RUNNING) AS AVG_UTILIZATION,
    MAX(wlh.AVG_RUNNING) AS MAX_UTILIZATION,
    COUNT(*) AS MEASUREMENT_COUNT
  FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_LOAD_HISTORY wlh
  JOIN (
    SELECT DISTINCT WAREHOUSE_NAME, WAREHOUSE_SIZE
    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
    WHERE START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
      AND WAREHOUSE_NAME IS NOT NULL
  ) wh ON wlh.WAREHOUSE_NAME = wh.WAREHOUSE_NAME
  WHERE wlh.START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
  GROUP BY wh.WAREHOUSE_NAME, wh.WAREHOUSE_SIZE
)
SELECT 
  WAREHOUSE_NAME,
  WAREHOUSE_SIZE,
  ROUND(AVG_UTILIZATION * 100, 2) AS AVG_UTILIZATION_PCT,
  ROUND(MAX_UTILIZATION * 100, 2) AS MAX_UTILIZATION_PCT,
  MEASUREMENT_COUNT,
  CASE 
    WHEN AVG_UTILIZATION < 0.40 THEN 'UNDERUTILIZED'
    WHEN AVG_UTILIZATION > 0.80 THEN 'OVERUTILIZED'
    ELSE 'OPTIMAL'
  END AS UTILIZATION_STATUS
FROM warehouse_stats
ORDER BY AVG_UTILIZATION_PCT ASC;


-- ============================================
-- 2. AUTO-SUSPEND OPPORTUNITIES
-- Purpose: Find warehouses with long idle times
-- ============================================

-- Query 2: Warehouse Idle Time Analysis
WITH query_timeline AS (
  SELECT 
    WAREHOUSE_NAME,
    START_TIME,
    END_TIME,
    LEAD(START_TIME) OVER (PARTITION BY WAREHOUSE_NAME ORDER BY START_TIME) AS NEXT_QUERY_START,
    DATEDIFF(minute, END_TIME, LEAD(START_TIME) OVER (PARTITION BY WAREHOUSE_NAME ORDER BY START_TIME)) AS IDLE_MINUTES
  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
  WHERE START_TIME >= DATEADD(day, -7, CURRENT_TIMESTAMP())
    AND WAREHOUSE_NAME IS NOT NULL
    AND END_TIME IS NOT NULL
)
SELECT 
  WAREHOUSE_NAME,
  COUNT(*) AS QUERY_COUNT,
  AVG(IDLE_MINUTES) AS AVG_IDLE_MINUTES,
  MAX(IDLE_MINUTES) AS MAX_IDLE_MINUTES,
  PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY IDLE_MINUTES) AS MEDIAN_IDLE_MINUTES,
  SUM(CASE WHEN IDLE_MINUTES > 5 THEN 1 ELSE 0 END) AS IDLE_PERIODS_OVER_5MIN,
  CASE 
    WHEN AVG(IDLE_MINUTES) > 5 THEN 'ENABLE_AUTO_SUSPEND'
    ELSE 'OPTIMAL'
  END AS RECOMMENDATION
FROM query_timeline
WHERE IDLE_MINUTES IS NOT NULL
GROUP BY WAREHOUSE_NAME
HAVING COUNT(*) > 10
ORDER BY AVG_IDLE_MINUTES DESC;


-- ============================================
-- 3. QUERY REPETITION (for caching recommendations)
-- Purpose: Find frequently repeated queries for result caching
-- ============================================

-- Query 3: Repeated Queries Analysis
WITH query_patterns AS (
  SELECT 
    QUERY_TEXT,
    DATABASE_NAME,
    SCHEMA_NAME,
    WAREHOUSE_NAME,
    USER_NAME,
    COUNT(*) AS EXECUTION_COUNT,
    SUM(TOTAL_ELAPSED_TIME) AS TOTAL_EXECUTION_TIME_MS,
    AVG(TOTAL_ELAPSED_TIME) AS AVG_EXECUTION_TIME_MS,
    SUM(BYTES_SCANNED) AS TOTAL_BYTES_SCANNED,
    MIN(START_TIME) AS FIRST_EXECUTION,
    MAX(START_TIME) AS LAST_EXECUTION
  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
  WHERE START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
    AND QUERY_TEXT IS NOT NULL
    AND QUERY_TYPE IN ('SELECT', 'SHOW', 'DESCRIBE')
    AND EXECUTION_STATUS = 'SUCCESS'
  GROUP BY QUERY_TEXT, DATABASE_NAME, SCHEMA_NAME, WAREHOUSE_NAME, USER_NAME
  HAVING COUNT(*) >= 10
)
SELECT 
  LEFT(QUERY_TEXT, 200) AS QUERY_PREVIEW,
  DATABASE_NAME,
  SCHEMA_NAME,
  WAREHOUSE_NAME,
  USER_NAME,
  EXECUTION_COUNT,
  ROUND(TOTAL_EXECUTION_TIME_MS / 1000, 2) AS TOTAL_EXECUTION_TIME_SEC,
  ROUND(AVG_EXECUTION_TIME_MS / 1000, 2) AS AVG_EXECUTION_TIME_SEC,
  ROUND(TOTAL_BYTES_SCANNED / 1073741824, 2) AS TOTAL_GB_SCANNED,
  FIRST_EXECUTION,
  LAST_EXECUTION,
  'ENABLE_RESULT_CACHE' AS RECOMMENDATION
FROM query_patterns
ORDER BY EXECUTION_COUNT DESC, TOTAL_EXECUTION_TIME_MS DESC
LIMIT 50;


-- ============================================
-- 4. AUTOMATIC CLUSTERING ANALYSIS
-- Purpose: Identify clustering costs and benefits
-- ============================================

-- Query 4: Clustering Cost vs Benefit Analysis
SELECT 
  DATABASE_NAME,
  SCHEMA_NAME,
  TABLE_NAME,
  START_TIME,
  END_TIME,
  CREDITS_USED,
  NUM_BYTES_RECLUSTERED,
  NUM_ROWS_RECLUSTERED
FROM SNOWFLAKE.ACCOUNT_USAGE.AUTOMATIC_CLUSTERING_HISTORY
WHERE START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
ORDER BY CREDITS_USED DESC
LIMIT 100;

-- Expected columns:
-- DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, START_TIME, END_TIME, 
-- CREDITS_USED, NUM_BYTES_RECLUSTERED, NUM_ROWS_RECLUSTERED


-- Query 4B: High Clustering Cost Tables
WITH clustering_costs AS (
  SELECT 
    DATABASE_NAME,
    SCHEMA_NAME,
    TABLE_NAME,
    SUM(CREDITS_USED) AS TOTAL_CLUSTERING_CREDITS,
    COUNT(*) AS CLUSTERING_OPERATIONS,
    SUM(NUM_BYTES_RECLUSTERED) AS TOTAL_BYTES_RECLUSTERED,
    MAX(START_TIME) AS LAST_CLUSTERING
  FROM SNOWFLAKE.ACCOUNT_USAGE.AUTOMATIC_CLUSTERING_HISTORY
  WHERE START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
  GROUP BY DATABASE_NAME, SCHEMA_NAME, TABLE_NAME
),
query_performance AS (
  SELECT 
    DATABASE_NAME,
    SCHEMA_NAME,
    COUNT(*) AS QUERY_COUNT,
    AVG(TOTAL_ELAPSED_TIME) AS AVG_QUERY_TIME_MS
  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
  WHERE START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
    AND DATABASE_NAME IS NOT NULL
    AND SCHEMA_NAME IS NOT NULL
  GROUP BY DATABASE_NAME, SCHEMA_NAME
)
SELECT 
  cc.DATABASE_NAME,
  cc.SCHEMA_NAME,
  cc.TABLE_NAME,
  cc.TOTAL_CLUSTERING_CREDITS,
  cc.CLUSTERING_OPERATIONS,
  ROUND(cc.TOTAL_BYTES_RECLUSTERED / 1073741824, 2) AS TOTAL_GB_RECLUSTERED,
  cc.LAST_CLUSTERING,
  COALESCE(qp.QUERY_COUNT, 0) AS SCHEMA_QUERY_COUNT,
  COALESCE(ROUND(qp.AVG_QUERY_TIME_MS / 1000, 2), 0) AS AVG_QUERY_TIME_SEC,
  CASE 
    WHEN cc.TOTAL_CLUSTERING_CREDITS > 10 AND COALESCE(qp.QUERY_COUNT, 0) < 100 
      THEN 'CONSIDER_DISABLING_CLUSTERING'
    ELSE 'CLUSTERING_JUSTIFIED'
  END AS RECOMMENDATION
FROM clustering_costs cc
LEFT JOIN query_performance qp 
  ON cc.DATABASE_NAME = qp.DATABASE_NAME 
  AND cc.SCHEMA_NAME = qp.SCHEMA_NAME
ORDER BY cc.TOTAL_CLUSTERING_CREDITS DESC;


-- ============================================
-- 5. MATERIALIZED VIEW REFRESH COSTS
-- Purpose: Track materialized view refresh costs
-- ============================================

-- Query 5: Materialized View Refresh History
SELECT 
  DATABASE_NAME,
  SCHEMA_NAME,
  START_TIME,
  END_TIME,
  CREDITS_USED
FROM SNOWFLAKE.ACCOUNT_USAGE.MATERIALIZED_VIEW_REFRESH_HISTORY
WHERE START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
ORDER BY CREDITS_USED DESC
LIMIT 100;

-- Expected columns:
-- DATABASE_NAME, SCHEMA_NAME, START_TIME, END_TIME, CREDITS_USED
-- Note: BYTES_WRITTEN and ROWS_WRITTEN not available in MATERIALIZED_VIEW_REFRESH_HISTORY


-- ============================================
-- 6. TASK EXECUTION COSTS
-- Purpose: Track scheduled task costs
-- ============================================

-- Query 6: Task History and Costs
SELECT 
  DATABASE_NAME,
  SCHEMA_NAME,
  NAME AS TASK_NAME,
  STATE,
  SCHEDULED_TIME,
  COMPLETED_TIME,
  QUERY_START_TIME,
  QUERY_ID,
  ERROR_CODE,
  ERROR_MESSAGE
FROM SNOWFLAKE.ACCOUNT_USAGE.TASK_HISTORY
WHERE SCHEDULED_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
ORDER BY SCHEDULED_TIME DESC
LIMIT 100;

-- Expected columns:
-- DATABASE_NAME, SCHEMA_NAME, TASK_NAME, STATE, SCHEDULED_TIME, 
-- COMPLETED_TIME, QUERY_START_TIME, QUERY_ID, ERROR_CODE, ERROR_MESSAGE


-- Query 6B: Task Cost Analysis (combining with query history)
WITH task_queries AS (
  SELECT 
    th.DATABASE_NAME,
    th.SCHEMA_NAME,
    th.NAME AS TASK_NAME,
    th.SCHEDULED_TIME,
    th.QUERY_ID,
    qh.WAREHOUSE_NAME,
    qh.TOTAL_ELAPSED_TIME,
    qh.EXECUTION_STATUS
  FROM SNOWFLAKE.ACCOUNT_USAGE.TASK_HISTORY th
  LEFT JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY qh 
    ON th.QUERY_ID = qh.QUERY_ID
  WHERE th.SCHEDULED_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
)
SELECT 
  DATABASE_NAME,
  SCHEMA_NAME,
  TASK_NAME,
  COUNT(*) AS EXECUTION_COUNT,
  SUM(CASE WHEN EXECUTION_STATUS = 'SUCCESS' THEN 1 ELSE 0 END) AS SUCCESSFUL_RUNS,
  SUM(CASE WHEN EXECUTION_STATUS = 'FAILED' THEN 1 ELSE 0 END) AS FAILED_RUNS,
  AVG(TOTAL_ELAPSED_TIME) AS AVG_EXECUTION_TIME_MS,
  MAX(TOTAL_ELAPSED_TIME) AS MAX_EXECUTION_TIME_MS,
  WAREHOUSE_NAME
FROM task_queries
GROUP BY DATABASE_NAME, SCHEMA_NAME, TASK_NAME, WAREHOUSE_NAME
ORDER BY EXECUTION_COUNT DESC;


-- ============================================
-- 7. COMPREHENSIVE WASTE DETECTION (Enhanced)
-- Purpose: Find all archival candidates with cost impact
-- ============================================

-- Query 7: Enhanced Table Archival Candidates
WITH table_storage AS (
  SELECT 
    TABLE_CATALOG,
    TABLE_SCHEMA,
    TABLE_NAME,
    ACTIVE_BYTES,
    TIME_TRAVEL_BYTES,
    FAILSAFE_BYTES,
    IS_TRANSIENT,
    COALESCE(TABLE_DROPPED, TABLE_CREATED) AS LAST_MODIFIED
  FROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS
  WHERE DELETED = FALSE
    AND TABLE_CATALOG IS NOT NULL
),
table_access AS (
  SELECT 
    obj.value:objectName::STRING AS TABLE_NAME,
    MAX(QUERY_START_TIME) AS LAST_ACCESS
  FROM SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY,
  LATERAL FLATTEN(input => DIRECT_OBJECTS_ACCESSED) obj
  WHERE QUERY_START_TIME >= DATEADD(day, -90, CURRENT_TIMESTAMP())
    AND obj.value:objectDomain::STRING = 'Table'
  GROUP BY obj.value:objectName::STRING
)
SELECT 
  ts.TABLE_CATALOG AS DATABASE_NAME,
  ts.TABLE_SCHEMA AS SCHEMA_NAME,
  ts.TABLE_NAME,
  ROUND(ts.ACTIVE_BYTES / 1073741824, 2) AS ACTIVE_GB,
  ROUND(ts.TIME_TRAVEL_BYTES / 1073741824, 2) AS TIME_TRAVEL_GB,
  ROUND(ts.FAILSAFE_BYTES / 1073741824, 2) AS FAILSAFE_GB,
  ROUND((ts.ACTIVE_BYTES + ts.TIME_TRAVEL_BYTES + ts.FAILSAFE_BYTES) / 1073741824, 2) AS TOTAL_GB,
  ts.IS_TRANSIENT,
  ts.LAST_MODIFIED,
  ta.LAST_ACCESS,
  DATEDIFF(day, COALESCE(ta.LAST_ACCESS, ts.LAST_MODIFIED), CURRENT_TIMESTAMP()) AS DAYS_SINCE_ACCESS,
  ROUND(((ts.ACTIVE_BYTES + ts.TIME_TRAVEL_BYTES + ts.FAILSAFE_BYTES) / 1099511627776) * 23, 2) AS MONTHLY_COST_USD,
  CASE 
    WHEN DATEDIFF(day, COALESCE(ta.LAST_ACCESS, ts.LAST_MODIFIED), CURRENT_TIMESTAMP()) > 90 
      AND (ts.ACTIVE_BYTES / 1073741824) > 100 
      THEN 'HIGH_PRIORITY_ARCHIVE'
    WHEN DATEDIFF(day, COALESCE(ta.LAST_ACCESS, ts.LAST_MODIFIED), CURRENT_TIMESTAMP()) > 90 
      AND (ts.ACTIVE_BYTES / 1073741824) > 10 
      THEN 'MEDIUM_PRIORITY_ARCHIVE'
    WHEN DATEDIFF(day, COALESCE(ta.LAST_ACCESS, ts.LAST_MODIFIED), CURRENT_TIMESTAMP()) > 90 
      THEN 'LOW_PRIORITY_ARCHIVE'
    ELSE 'ACTIVE'
  END AS RECOMMENDATION_PRIORITY
FROM table_storage ts
LEFT JOIN table_access ta ON ts.TABLE_NAME = ta.TABLE_NAME
WHERE COALESCE(ta.LAST_ACCESS, ts.LAST_MODIFIED) < DATEADD(day, -90, CURRENT_TIMESTAMP())
ORDER BY MONTHLY_COST_USD DESC, DAYS_SINCE_ACCESS DESC
LIMIT 100;


-- ============================================
-- 8. ROI TRACKING BASELINE
-- Purpose: Establish baseline metrics for ROI calculation
-- ============================================

-- Query 8: Current Monthly Baseline Costs
SELECT 
  'COMPUTE' AS COST_CATEGORY,
  SUM(CREDITS_USED_COMPUTE + CREDITS_USED_CLOUD_SERVICES) AS TOTAL_CREDITS,
  SUM(CREDITS_USED_COMPUTE + CREDITS_USED_CLOUD_SERVICES) * 3 AS ESTIMATED_COST_USD
FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY
WHERE START_TIME >= DATE_TRUNC('month', CURRENT_DATE())
UNION ALL
SELECT 
  'STORAGE' AS COST_CATEGORY,
  NULL AS TOTAL_CREDITS,
  AVG(STORAGE_BYTES + STAGE_BYTES + FAILSAFE_BYTES) / 1099511627776 * 23 AS ESTIMATED_COST_USD
FROM SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE
WHERE USAGE_DATE >= DATE_TRUNC('month', CURRENT_DATE());


-- ============================================
-- TESTING CHECKLIST
-- ============================================
-- Run each query above and verify:
-- 1. Query executes without errors
-- 2. Returns expected columns
-- 3. Data looks reasonable (no nulls where unexpected)
-- 4. Performance is acceptable (< 30 seconds)
-- 
-- Note any errors or schema mismatches and report them
-- before we proceed with implementation.
-- ============================================
