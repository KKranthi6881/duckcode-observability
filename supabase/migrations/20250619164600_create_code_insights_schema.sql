   -- Step 1: Create the new schema to namespace our tables
   CREATE SCHEMA code_insights;

   GRANT USAGE ON SCHEMA code_insights TO postgres, service_role, authenticated, anon;

   -- Step 2: Table to track all files from GitHub that are candidates for parsing
   CREATE TABLE code_insights.files (
       id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       user_id UUID REFERENCES auth.users(id) NOT NULL, -- User who initiated the tracking or last manual processing
       repository_full_name TEXT NOT NULL, -- e.g., 'owner/repo'
       file_path TEXT NOT NULL, -- e.g., 'src/utils/helpers.py'
       file_hash TEXT, -- SHA-256 hash of the file content to detect changes
       language TEXT, -- e.g., 'sql_snowflake', 'python_pyspark', 'dbt'
       last_scanned_at TIMESTAMPTZ,
       parsing_status TEXT DEFAULT 'pending' NOT NULL, -- pending, processing, completed, failed, needs_reprocessing
       error_message TEXT,
       created_at TIMESTAMPTZ DEFAULT now() NOT NULL,
       updated_at TIMESTAMPTZ DEFAULT now() NOT NULL,
       UNIQUE(repository_full_name, file_path) -- Ensures a file path within a repo is tracked only once
   );
   COMMENT ON TABLE code_insights.files IS 'Central registry for all source code files tracked by the system. Uniqueness is per repo and file path.';

   -- Step 3: Table to manage the agentic processing jobs, acting as a queue
   CREATE TABLE code_insights.processing_jobs (
       id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       file_id UUID REFERENCES code_insights.files(id) ON DELETE CASCADE NOT NULL,
       job_type TEXT NOT NULL, -- 'code_summary', 'data_lineage', 'knowledge_graph'
       status TEXT NOT NULL DEFAULT 'queued', -- queued, running, completed, failed
       attempts INT DEFAULT 0,
       last_attempt_at TIMESTAMPTZ,
       llm_model_used TEXT, -- e.g., 'gpt-4-turbo-preview'
       prompt_tokens INT,
       completion_tokens INT,
       total_tokens INT,
       processing_duration_ms INT,
       error_details TEXT,
       created_at TIMESTAMPTZ DEFAULT now() NOT NULL,
       updated_at TIMESTAMPTZ DEFAULT now() NOT NULL
   );
   COMMENT ON TABLE code_insights.processing_jobs IS 'Queue for managing asynchronous LLM processing tasks for each file and job type.';

   -- Step 4: Table for detailed, structured code summaries
   CREATE TABLE code_insights.code_summaries (
       id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       job_id UUID REFERENCES code_insights.processing_jobs(id) ON DELETE CASCADE NOT NULL UNIQUE, -- Ensures one summary per successful job
       file_id UUID REFERENCES code_insights.files(id) ON DELETE CASCADE NOT NULL,
       summary_json JSONB NOT NULL, -- Contains business context, logic, code blocks, etc.
       created_at TIMESTAMPTZ DEFAULT now() NOT NULL
   );
   COMMENT ON TABLE code_insights.code_summaries IS 'Stores structured JSON summaries of code logic and business purpose, generated by an LLM.';

   -- Step 5: For Lineage & Knowledge Graphs, a graph data model is best
   -- Nodes can be tables, columns, views, CTEs, scripts, etc.
   CREATE TABLE code_insights.graph_nodes (
       id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       file_id UUID REFERENCES code_insights.files(id) ON DELETE CASCADE NOT NULL,
       job_id UUID REFERENCES code_insights.processing_jobs(id) ON DELETE CASCADE, -- Link to the job that created/updated this node
       node_type TEXT NOT NULL, -- 'table', 'column', 'view', 'cte', 'script', 'file', 'function'
       node_name TEXT NOT NULL, -- Fully qualified name if possible, e.g., 'schema.table.column' or 'function_name'
       properties JSONB, -- e.g., { "schema": "public", "datatype": "varchar", "description": "...", "start_line": 10, "end_line": 25 }
       created_at TIMESTAMPTZ DEFAULT now() NOT NULL,
       -- Consider a unique constraint based on file_id, node_type, and node_name if appropriate for your use case
       UNIQUE(file_id, node_type, node_name)
   );
   COMMENT ON TABLE code_insights.graph_nodes IS 'Stores all entities (tables, columns, functions, files etc.) as nodes in a graph, identified by LLM parsing.';

   -- Step 6: Edges represent the relationships between nodes (lineage, calls, dependencies)
   CREATE TABLE code_insights.graph_edges (
       id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       job_id UUID REFERENCES code_insights.processing_jobs(id) ON DELETE CASCADE, -- Link to the job that created/updated this edge
       source_node_id UUID REFERENCES code_insights.graph_nodes(id) ON DELETE CASCADE NOT NULL,
       target_node_id UUID REFERENCES code_insights.graph_nodes(id) ON DELETE CASCADE NOT NULL,
       relationship_type TEXT NOT NULL, -- 'reads_from', 'writes_to', 'joins_with', 'calls_function', 'imports_file'
       properties JSONB, -- e.g., { "join_type": "inner", "on_columns": ["id"], "transformation_logic": "...", "confidence_score": 0.9 }
       created_at TIMESTAMPTZ DEFAULT now() NOT NULL
   );
   COMMENT ON TABLE code_insights.graph_edges IS 'Stores the relationships (lineage, dependencies, calls) between nodes, identified by LLM parsing.';

   -- Step 7: Table to store and version control our prompts
   CREATE TABLE code_insights.prompt_templates (
       id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       prompt_type TEXT NOT NULL, -- 'language_specific', 'task_specific'
       prompt_key TEXT NOT NULL UNIQUE, -- 'tsql', 'snowflake', 'data_lineage', 'code_summary'
       template_content TEXT NOT NULL,
       version INT DEFAULT 1,
       is_active BOOLEAN DEFAULT true,
       description TEXT,
       created_at TIMESTAMPTZ DEFAULT now() NOT NULL,
       updated_at TIMESTAMPTZ DEFAULT now() NOT NULL
   );
   COMMENT ON TABLE code_insights.prompt_templates IS 'Manages versioned LLM prompts, allowing for easy updates and A/B testing without code deployments.';

-- Insert initial prompt templates (examples and placeholders)
INSERT INTO code_insights.prompt_templates (prompt_type, prompt_key, template_content, description) VALUES
('language_specific', 'generic_sql', 'You are an expert data engineer. The following script is written in a SQL dialect. Analyze it carefully.', 'Default prompt for generic SQL.'),
('language_specific', 'snowflake', 'You are an expert data engineer specializing in Snowflake SQL. You understand its specific syntax, including streams, tasks, stages, UDFs, stored procedures, and VARIANT data types. Pay close attention to these features when analyzing the script.', 'Prompt for Snowflake SQL specifics.'),
('language_specific', 'tsql', 'You are an expert in Transact-SQL (T-SQL) as used in Microsoft SQL Server. You understand its specific syntax, including common table expressions (CTEs), temp tables (#temptable, ##temptable), stored procedures, user-defined functions, triggers, and proprietary functions like PIVOT, UNPIVOT, and FOR XML. Analyze the script with these T-SQL nuances in mind.', 'Prompt for T-SQL specifics.'),
('language_specific', 'python_pyspark', 'You are an expert Python developer specializing in PySpark. You understand DataFrame operations, RDDs, Spark SQL, UDFs, and common data manipulation patterns in Spark. Analyze the script focusing on PySpark logic.', 'Prompt for PySpark specifics.'),
('language_specific', 'dbt', 'You are an expert in dbt (data build tool). You understand dbt models, sources, seeds, tests, macros, and Jinja templating within SQL. Analyze the following dbt model file, paying attention to ref() and source() functions, config blocks, and Jinja usage.', 'Prompt for dbt model specifics.'),
('language_specific', 'plsql', 'You are an expert in PL/SQL as used in Oracle databases. You understand its block structure, packages, procedures, functions, cursors, exception handling, and built-in packages. Analyze the script with these PL/SQL nuances in mind.', 'Prompt for PL/SQL specifics.'),
('language_specific', 'mysql', 'You are an expert in MySQL. You understand its specific SQL dialect, storage engines, stored procedures, functions, triggers, and common extensions. Analyze the script accordingly.', 'Prompt for MySQL specifics.'),
('language_specific', 'postgres', 'You are an expert in PostgreSQL. You understand its specific SQL dialect, advanced data types (JSONB, arrays), functions, stored procedures, extensions like PostGIS, and procedural languages like PL/pgSQL. Analyze the script accordingly.', 'Prompt for PostgreSQL specifics.'),

('task_specific', 'code_summary', 'Your task is to provide a comprehensive summary of the provided script. The summary should be structured as a valid JSON object. The JSON object must contain these top-level keys: "overall_purpose" (string: a concise explanation of what the entire script does), "business_context" (string: describe the business problem or domain this script addresses), "key_logic_sections" (array of objects: each object representing a distinct logic block with "section_title", "description", and "code_block" string properties). Ensure all string values are well-explained and detailed. Do not include any text outside the main JSON object.', 'Prompt for generating code summaries in JSON format.'),
('task_specific', 'data_lineage_column_level', 'Your task is to analyze the provided script and extract detailed column-level data lineage. Identify all source objects (tables, views, CTEs, files), intermediate transformations, and target objects. Provide your output as a single, valid JSON object. The JSON object must have a top-level key "lineage_paths", which is an array of objects. Each object in "lineage_paths" must have "source_column_fqn" (string: fully_qualified_name like db.schema.table.column or cte.column), "target_column_fqn" (string: fully_qualified_name), and "transformations" (array of strings: describing steps to get from source to target). If a direct mapping is not clear, describe the transformation as best as possible. Include all columns involved. Do not include any text outside the JSON object.', 'Prompt for extracting column-level data lineage in JSON format.'),
('task_specific', 'schema_extraction', 'Your task is to analyze the provided script and extract all schema definitions (e.g., CREATE TABLE, CREATE VIEW, ALTER TABLE). For each object, identify its name, type (table, view, etc.), columns (with data types), constraints (primary key, foreign key, unique, check), and any comments or descriptions. Output as a valid JSON object. The primary key should be "extracted_schemas", an array of objects, where each object has "object_name", "object_type", "columns" (array), "constraints" (array). Do not include any text outside the JSON object.', 'Prompt for extracting schema definitions (tables, views, columns, constraints).'),
('task_specific', 'code_segment_analysis', 'Your task is to break down the provided script into logical code segments or blocks. For each segment, provide: a "segment_title", a "detailed_explanation" of its logic and purpose, the "start_line" and "end_line" numbers, "input_variables" used, "output_variables" or results produced, and any "functions_called" or "external_dependencies". Output as a valid JSON object with a top-level key "code_segments", which is an array of these segment objects. Do not include any text outside the JSON object.', 'Prompt for analyzing distinct code blocks/segments within a file.');

   -- Step 8: Grant permissions to relevant roles

   -- Ensure the 'postgres' user (or the role executing migrations) owns the schema
   -- This is usually implicit if 'postgres' runs the migration, but let's be clear.
   -- If your migrations run as a different user, adjust 'postgres' accordingly.
   ALTER SCHEMA code_insights OWNER TO postgres;

   -- Grant USAGE on the schema to service_role
   GRANT USAGE ON SCHEMA code_insights TO service_role;
   GRANT USAGE ON SCHEMA code_insights TO postgres; -- postgres should already have it as owner

   -- Grant ALL privileges on ALL TABLES in the schema to service_role
   GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA code_insights TO service_role;
   GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA code_insights TO postgres; -- postgres as owner

   -- Grant ALL privileges on ALL SEQUENCES in the schema to service_role (if any were used)
   GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA code_insights TO service_role;
   GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA code_insights TO postgres;

   -- Grant EXECUTE on ALL FUNCTIONS in the schema to service_role (if any were used)
   GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA code_insights TO service_role;
   GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA code_insights TO postgres;

   -- CRITICAL: Alter default privileges for the 'postgres' user (or migration runner)
   -- This ensures that any NEW objects created in this schema by 'postgres'
   -- will automatically grant these privileges to 'service_role'.
   ALTER DEFAULT PRIVILEGES FOR ROLE postgres IN SCHEMA code_insights
      GRANT ALL ON TABLES TO service_role;

   ALTER DEFAULT PRIVILEGES FOR ROLE postgres IN SCHEMA code_insights
      GRANT ALL ON SEQUENCES TO service_role;

   ALTER DEFAULT PRIVILEGES FOR ROLE postgres IN SCHEMA code_insights
      GRANT EXECUTE ON FUNCTIONS TO service_role;

   -- Also, ensure service_role itself has these default privileges for objects it might create (less likely in this flow)
   ALTER DEFAULT PRIVILEGES FOR ROLE service_role IN SCHEMA code_insights
      GRANT ALL ON TABLES TO service_role;

   ALTER DEFAULT PRIVILEGES FOR ROLE service_role IN SCHEMA code_insights
      GRANT ALL ON SEQUENCES TO service_role;

   ALTER DEFAULT PRIVILEGES FOR ROLE service_role IN SCHEMA code_insights
      GRANT EXECUTE ON FUNCTIONS TO service_role;

   -- Re-grant to be absolutely sure after default changes
   GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA code_insights TO service_role;

   -- Create indexes for performance on frequently queried columns
   CREATE INDEX idx_files_repo_path ON code_insights.files(repository_full_name, file_path);
   CREATE INDEX idx_files_user_id ON code_insights.files(user_id);
   CREATE INDEX idx_jobs_file_id ON code_insights.processing_jobs(file_id);
   CREATE INDEX idx_jobs_status_type ON code_insights.processing_jobs(status, job_type);
   CREATE INDEX idx_summaries_file_id ON code_insights.code_summaries(file_id);
   CREATE INDEX idx_nodes_file_id ON code_insights.graph_nodes(file_id);
   CREATE INDEX idx_nodes_type_name ON code_insights.graph_nodes(node_type, node_name);
   CREATE INDEX idx_edges_source_node_id ON code_insights.graph_edges(source_node_id);
   CREATE INDEX idx_edges_target_node_id ON code_insights.graph_edges(target_node_id);